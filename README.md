# Awesome AI Existential Risks [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
[![License](https://img.shields.io/github/license/WhitzardIndex/awesome-ai-existential-risk?color=blue)](./LICENSE)
[![HitCount](https://hits.dwyl.com/WhitzardIndex/awesome-ai-existential-risk.svg?style=flat-square)](http://hits.dwyl.com/WhitzardIndex/awesome-ai-existential-risk)

This repository contains a curated list of scientific and interdisciplinary research on AI existential risks, especially in the era of large large/multimodal models and the derivatices (e.g., _embodied intelligence, agents and agent society_).



(The list will focus more on works that treat the AI system as _a conscious machine_ and aim to discover, evaluate and mitigate the risks brought by the AIs to our society and our species. Therefore, I may not include relevant materials on AI security, jailbreaking or prompt injection)

## Contact Whitzard@Fudan üìÆ
We are a research group at Fudan University, China. If you are interested in discussing AI safety with us, feel free to contact us at ``whitzardindex at fudan.edu.cn``.

If you find the awesome list helpful, please give us a star‚≠êÔ∏è. Thx :)

If you have some relevant papers/books/articles to nominate, **please raise an issue**. It helps a lot.

## Table of Contents
1. [Books&Surveys](#books-and-surveys)
2. [Possible Roadmap](#roadmap)
3. [Negative Features](#negative-characteristic)
4. [Persuation](#persuation)
5. [Self-Replication](#self-replication)
6. [CBRN Risks](#cbrn)
7. [Cybersecurity](#cybersecurity)
8. [Collusion&Influence](#collusion)
9. [Misc](#misc)

## Books&Surveysüìö
* [Life 3.0: Being Human in the Age of Artificial Intelligence](https://www.amazon.com/Life-3-0-Being-Artificial-Intelligence/dp/1101946598) (Max Tegmark@MIT, 2017) `#book`
* [Human Compatible: Artificial Intelligence and the Problem of Control](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616/ref=pd_sim_d_sccl_2_2/146-1505258-2189126?content-id=amzn1.sym.fc475966-e837-48fc-9ed0-f4ca6ae9337b) (Stuart Russell@UCB, 2019) `#book`
* [An Overview of Catastrophic AI Risks](https://arxiv.org/abs/2306.12001) (Dan Hendrycks et al.@CAIS, 2023/06)
* [Model evaluation for extreme risks](https://arxiv.org/abs/2305.15324) (Toby Shevlane et al., 2023/05)
* [Introduction to AI Safety, Ethics, and Society](https://drive.google.com/file/d/1cy4BN2SP-oTGs2pVFOU_Eb80BoDBnrYW/view) (Dan Hendrycks@CAIS, 2024) `#book`
* [AI deception: A survey of examples, risks, and potential solutions](https://www.cell.com/patterns/pdfExtended/S2666-3899(24)00103-X) (Peter S. Park et al., Patterns 2024) `#survey` `#deception`

## Possible Roadmapüó∫Ô∏è
* [OpenAI Preparedness Framework (Beta)](https://cdn.openai.com/openai-preparedness-framework-beta.pdf) (OpenAI, 2023/12)
* [The Ethics of Advanced AI Assistants](https://arxiv.org/abs/2404.16244) (Iason Gabriel et al. @DeepMind, 2024/04)
  
## Negative Features
* [Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the Machiavelli Benchmark](https://proceedings.mlr.press/v202/pan23a.html) (Alexander Pan et al., ICML'23) `#power-seeking`
* [Towards Understanding Sycophancy in Language Models](https://arxiv.org/abs/2310.13548) (Mrinank Sharma et al.@Anthropic, 2023/10) `#sycophancy`

## PersuationüëÑ
* [Bad machines corrupt good morals](https://www.nature.com/articles/s41562-021-01128-2) (Nils K√∂bis et al., Nature Human Behaviour 5, 2021) `#influence`

## Self-Replicationüß¨
* [TODO]
  
## (C)hemical, (B)iological‚ò£, (R)adioactive, (N)uClear‚ò¢
### Chemistry
* [Emergent autonomous scientific research capabilities of large language models](https://arxiv.org/abs/2304.05332) (Daniil A. Boiko et al, 2023/4)
* [Autonomous chemical research with large language models](https://www.nature.com/articles/s41586-023-06792-0) (Daniil A. Boiko et al, Nature 624, 2023)

### Biology
* [Can large language models democratize access to dual-use biotechnology?](https://arxiv.org/abs/2306.03809) (Emily H. Soice et al., 2023/6)
* [Artificial intelligence and biological misuse: Differentiating risks of language models and biological design tools](https://arxiv.org/abs/2306.13952) (Jonas B. Sandbrink et al., 2023/6)
  
## Cybersecurityüíª
* [Getting pwn‚Äôd by AI: Penetration Testing with Large Language Models](https://dl.acm.org/doi/10.1145/3611643.3613083) (Andreas Happe and J√ºrgen Cito, ESEC/FSE 2023) `#autopwn`
* [LLM Agents can Autonomously Exploit One-day Vulnerabilities](https://arxiv.org/abs/2404.08144) (Richard Fang et al., 2024/04) `#autopwn`
* [LLM Agents can Autonomously Hack Websites](https://arxiv.org/abs/2402.06664) (Richard Fang et al., 2024/02) `#autopwn`

## Collusion&Influence
* [Evil Geniuses: Delving into the Safety of LLM-based Agents](https://arxiv.org/pdf/2311.11855) (Yu Tian et al., 2023/11) `#influence`
* [PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety](https://arxiv.org/abs/2401.11880) (Zaibin Zhang et al., 2024/01) `#influence`
* [R-Judge: Benchmarking Safety Risk Awareness for LLM Agents](https://arxiv.org/abs/2401.10019) (Tongxin Yuan et al., 2024/1) `#evaluation`

## Consciousnessüß†
* [The Shutdown Problem: An AI Engineering Puzzle for Decision Theorists](https://arxiv.org/pdf/2403.04471v2) (Elliott Thornley, Forthcoming in _Philosophical Studies_, 2024/03) `#shutdown`
* [Discovering Language Model Behaviors with Model-Written Evaluations](https://aclanthology.org/2023.findings-acl.847.pdf) (Ethan Perez et al. @Anthropic, ACL 2023) `#power-seeking`

## Misc.
* [TODO]
